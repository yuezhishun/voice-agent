## 目标与原则

### 目标

- **边说边出字**：低延迟（用户实时看到字幕）。
- **最终更准**：说完后用更强模型做定稿纠错（2-pass）。
- **不卡顿、不爆算力**：避免“越说越慢”的全量重识别。

### 原则

1. **VAD 不是一个模型就能解决的**：要做成“endpointing 状态机”（VAD + 规则融合）。
2. **2-pass 不全量重跑**：只对“最近窗口”重解码 / 纠错，并对更早文本“冻结”。

------

## 系统结构（模块分工）

### A. 音频输入与前处理（强建议有）

- 重采样到 16k、单声道
- AGC（谨慎）、降噪（RNNoise/NS）、必要时 AEC（通话/会议回声场景）
- 目的：提升 SNR，减少“声学不可辨”导致的硬错误（这类错误 2-pass 很难救）

------

### B. Endpointing（端点判定系统，解决你“复杂环境 VAD 不准”的关键）

你可以把它看成“切不切段”的总控制器，它综合：

- VAD 概率（来自模型自带 VAD 或外部 VAD）
- 静音时长
- 最短语音段/最短静音段
- 合并规则（防误切）
- 最大段长（防超长导致 2-pass 很慢）

**推荐一套默认参数（起步就能用）**

- `end_silence_ms`: 800ms（噪声越大越应该更长：900–1200ms）
- `min_segment_ms`: 1200ms（防止被碎切）
- `min_silence_ms`: 300ms（静音太短不切）
- `merge_back_ms`: 300ms（切完 300ms 内又持续说话 → 合并）
- `max_segment_ms`: 15000ms（15s 强制切，避免超长）

> 这一步决定了：你后面字幕是不是乱跳、2-pass 会不会越跑越慢。
> “VAD 不准”多数不是换个 VAD 就好，而是缺少这套状态机。

------

### C. 1-pass：流式 ASR（你选的 paraformer online-onnx）

职责：**低延迟输出**，让用户“边说边看到字幕”。

输出两类结果：

- **Partial（临时结果）**：可以频繁更新，但要有“稳定策略”
- **Final-segment（段落定稿）**：由 endpointing 判定“这段说完了”，就产出一段 final（可进入 2-pass）

**建议 chunk 方式**

- 音频推理步长：200–400ms/次（越小越实时，但算力更大；越大越稳但延迟更高）
- Partial 展示稳定策略：
  - 只展示“稳定前缀”（例如冻结除最后 1–2 秒以外的部分）
  - 避免字幕全句反复改字（用户体验会很差）

------

### D. 2-pass：离线强模型定稿（SenseVoice 或 Fun-ASR-Nano）

职责：**提高最终准确率**，特别是：

- 同音词消歧、术语/专名更稳
- 跨段边界错误更少
- 句子更顺（可选：标点、数字格式化等）

但关键是：**不要每次都把从 0 到现在全部重跑**，否则会越说越慢。

------

## 2-pass 的正确做法：窗口化定稿 + 前缀冻结（核心）

### 1) 前缀冻结（Prefix Lock）

- 一旦某段文本“稳定并且离当前足够远”，就**冻结**：后面不再修改它。
- 可选：允许非常小的回撤（例如只允许改最后 1 秒）来兼顾边界修正。

### 2) 只对“最近窗口”做 2-pass

- 窗口可以按时长或按段数：
  - `window_seconds`: 10–20s（推荐从 12s 起步）
  - 或 `window_segments`: 最近 2–4 个段
- 2-pass 只重跑窗口内音频（或做重打分），窗口之前的音频**不再重复计算**。

### 3) 用“文本上下文”喂 2-pass，而不是拼长音频

即使 2-pass 只跑窗口音频，也能利用上下文：

- 把“已冻结文本”作为 prompt/context（如果你的 2-pass 支持热词/上下文注入就更好）
- 这样往往能达到“长上下文纠错”的效果，但计算量小得多

------

## 触发 2-pass 的时机（比“累计 3 段就全量重识别”更稳）

建议优先使用：

1. **长静音触发**：静音 ≥ `end_silence_ms` 且通过 endpointing
2. **窗口满触发**：窗口 ≥ 12–20s（兜底，保证不会拖到很长才定稿）
3. **显式事件**：用户按下“结束说话/提交”、或者检测到明显话题结束

------

## 输出与对齐：如何让字幕“不乱跳”、时间戳“不漂”

这是混用两个模型最容易翻车的地方，建议这样做：

1. **2-pass 只改窗口内文本**：窗口外文本绝对不动（用户看到的历史字幕不会回撤一大片）
2. **“最小改动”策略**：把 2-pass 新文本与 1-pass 窗口文本做对齐（例如用编辑距离对齐），尽量少改
3. **时间戳处理**：
   - 如果你需要词级时间戳：建议在最终定稿后做 forced alignment（或用模型自带对齐能力），不要指望两套模型的时间戳天然一致
   - 如果只要段级时间戳：用 endpointing 产生的段起止更稳

------

## 一套你可以直接照搬的默认配置（起步版）

- Endpointing：
  - end_silence: 800ms（噪声更大用 1000–1200ms）
  - min_segment: 1200ms
  - merge_back: 300ms
  - max_segment: 15000ms
- 1-pass（流式）：
  - chunk: 320ms（或 400ms）
  - partial 回撤窗口：最后 1–2 秒允许变化，其余冻结展示
- 2-pass（离线定稿）：
  - window: 12 秒（或最近 3 段）
  - revise_scope：仅窗口内允许修改
  - prefix_lock：窗口前文本冻结
  - trigger：长静音 / 窗口满 / 用户提交

------

## 会得到什么效果、哪些地方别期待过高

### 明显提升（通常肉眼可见）

- 同音词/术语/专名更稳
- 边界处漏字/重复变少
- 最终稿可读性更好（配合标点/格式化）

### 不一定“拉满”的原因

- 如果主要错误来自强回声、强混响、SNR 极低：得先靠 AEC/降噪/麦阵等声学手段提升可辨性
- 如果 VAD 把语音整段漏掉：2-pass 也救不回来，所以 endpointing 仍是第一优先级

------

## 下一步怎么做（最省时间的落地顺序）

1. 先把 **endpointing 状态机**加进去（哪怕先用简单规则也行）
2. 跑通 1-pass 流式输出（partial + segment final）
3. 接上 2-pass，但只做“窗口化 + 冻结”，不要全量重跑
4. 最后再调参：end_silence、window_seconds、max_segment 等

