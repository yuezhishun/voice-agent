# PaddleSpeech 文本后处理策略详解

> 本文详细介绍 PaddleSpeech 中的文本后处理技术，包括标点符号恢复、文本规范化和文本纠错

## 目录

1. [标点符号恢复 (Punctuation Restoration)](#1-标点符号恢复-punctuation-restoration)
2. [文本规范化 (Text Normalization)](#2-文本规范化-text-normalization)
3. [文本纠错 (Text Correction)](#3-文本纠错-text-correction)
4. [完整后处理流水线](#4-完整后处理流水线)

---

## 1. 标点符号恢复 (Punctuation Restoration)

### 1.1 概述

标点符号恢复是 ASR 后处理的重要环节，为无标点的语音识别输出添加适当的标点符号，提高文本可读性。

**核心文件**:
- 模型: `paddlespeech/text/models/ernie_linear/ernie_linear.py`
- 推理: `paddlespeech/text/exps/ernie_linear/punc_restore.py`
- CLI: `paddlespeech/cli/text/infer.py`
- 数据集: `paddlespeech/text/models/ernie_linear/dataset.py`

### 1.2 模型架构

#### 1.2.1 ErnieLinear 模型

```python
# paddlespeech/text/models/ernie_linear/ernie_linear.py
class ErnieLinear(nn.Layer):
    """
    基于 ERNIE 的线性分类器，用于标点符号恢复

    架构:
        ERNIE Encoder → Linear Layer → Softmax → 标点预测
    """
    def __init__(self,
                 num_classes=None,           # 标点类别数
                 pretrained_token='ernie-1.0',  # 预训练模型
                 cfg_path=None,              # 配置文件路径
                 ckpt_path=None):             # 检查点路径

    def forward(self, input_ids, token_type_ids=None,
                position_ids=None, attention_mask=None):
        # ERNIE 编码
        y = self.ernie(input_ids, token_type_ids,
                       attention_mask, position_ids)
        # Reshape 和 Softmax
        y = paddle.reshape(y, shape=[-1, self.num_classes])
        logits = self.softmax(y)
        return y, logits
```

**架构图**:

```
输入文本
    ↓
[Tokenization] ErnieTokenizer (字级别)
    ↓
[ERNIE Encoder]
    ├─ 12 层 Transformer (ernie-1.0)
    ├─ Hidden Size: 768
    └─ Attention Heads: 12
    ↓
[Linear Layer] num_classes 个输出
    ↓
[Softmax] 概率分布
    ↓
[Argmax] 标点预测
    ↓
输出: 带标点的文本
```

#### 1.2.2 ErnieCrf 模型 (带 CRF)

```python
# paddlespeech/text/models/ernie_crf/model.py
class ErnieCrf(nn.Layer):
    """
    带 CRF 层的 ERNIE 模型，用于标点符号恢复

    优势:
        - 考虑标签之间的转移约束
        - 使用 Viterbi 解码找到最优路径
    """
    def __init__(self,
                 num_classes,
                 pretrained_token='ernie-1.0',
                 crf_lr=100):  # CRF 学习率倍数

    def forward(self, input_ids, token_type_ids=None,
                position_ids=None, attention_mask=None,
                lengths=None, labels=None):
        # ERNIE 编码
        logits = self.ernie(input_ids, token_type_ids,
                           attention_mask, position_ids)

        # Viterbi 解码
        _, prediction = self.viterbi_decoder(logits, lengths)

        if labels is not None:
            # CRF 损失计算
            loss = self.crf_loss(logits, lengths, labels)
            return loss, prediction
        return prediction
```

### 1.3 支持的标点符号

**中文标点符号** (vocab 文件定义):

```txt
  (空格)
， (逗号)
。 (句号)
？ (问号)
！ (感叹号)
； (分号)
： (冒号)
、 (顿号)
「 (左单引号)
」 (右单引号)
『 (左双引号)
』 (右双引号)
（ (左圆括号)
） (右圆括号)
```

**标点符号映射**:

| ID | 标点 | 使用场景 |
|----|------|---------|
| 0 | 空格 | 句子中停顿 |
| 1 | ， | 子句间停顿 |
| 2 | 。 | 句子结束 |
| 3 | ？ | 疑问句 |
| 4 | ！ | 感叹句 |
| 5 | ； | 并列分句 |
| 6 | ： | 解释说明 |
| 7 | 、 | 并列词语 |
| 8-13 | 引号括号 | 引用/注释 |

### 1.4 数据集格式

**训练数据格式** (带标点的文本):

```txt
床前明月光，疑是地上霜。
```

**Dataset 类解析**:

```python
# paddlespeech/text/models/ernie_linear/dataset.py
class PuncDatasetFromErnieTokenizer(Dataset):
    """
    将带标点的文本转换为训练样本

    处理流程:
        "床前明月光，疑是地上霜。"
        ↓ Tokenization
        ['床', '前', '明', '月', '光', '，', '疑', ...]
        ↓ 字符-标点配对
        ('床', ' '), ('前', ' '), ('光', '，'), ('疑', ' '), ('霜', '。')
        ↓ 编码
        input_ids: [token_ids...]
        labels: [punc_ids...]
    """
    def __init__(self, train_path, punc_path,
                 pretrained_token='ernie-1.0',
                 seq_len=100):
        # 加载标点词典
        self.punc2id = self.load_vocab(punc_path,
                                       extra_word_list=[" "])
        # 初始化 tokenizer
        self.tokenizer = ErnieTokenizer.from_pretrained(
            pretrained_token)
        # 预处理数据
        self.preprocess(self.txt_seqs)

    def preprocess(self, txt_seqs: list):
        """
        遍历文本序列，构建字-标点配对
        """
        for i in range(len(txt_seqs) - 1):
            word = txt_seqs[i]
            punc = txt_seqs[i + 1]

            if word in self.punc2id:
                continue  # 跳过标点

            # Tokenize 字符
            token = self.tokenizer(word)
            x = token["input_ids"][1:-1]  # 去掉 [CLS] [SEP]
            input_data.extend(x)

            # 标注每个 subword 的标点
            for _ in range(len(x) - 1):
                label.append(self.punc2id[" "])

            # 最后一个字符使用实际标点
            if punc in self.punc2id:
                label.append(self.punc2id[punc])
            else:
                label.append(self.punc2id[" "])
```

### 1.5 推理流程

**预处理**:

```python
def preprocess(text, punc_list, tokenizer):
    """
    清理文本并 tokenize
    """
    # 1. 清理文本
    clean_text = _clean_text(text, punc_list)
    # 移除非字母数字和中文，保留 A-Za-z0-9\u4e00-\u9fa5

    # 2. Tokenization (字级别)
    tokenized_input = tokenizer(
        list(clean_text),  # 字符列表
        return_length=True,
        is_split_into_words=True
    )

    return {
        'input_ids': tokenized_input['input_ids'],
        'seg_ids': tokenized_input['token_type_ids'],
        'seq_len': tokenized_input['seq_len']
    }
```

**模型推理**:

```python
@paddle.no_grad()
def infer():
    """
    模型推理
    """
    input_ids = paddle.to_tensor(_inputs['input_ids']).unsqueeze(0)
    seg_ids = paddle.to_tensor(_inputs['seg_ids']).unsqueeze(0)

    # 前向传播
    logits, _ = model(input_ids, seg_ids)

    # 获取预测
    preds = paddle.argmax(logits, axis=-1).squeeze(0)
    _outputs['preds'] = preds
```

**后处理**:

```python
def postprocess(input_ids, seq_len, preds, punc_list):
    """
    将预测结果转换为带标点的文本
    """
    # 获取 tokens (去掉 [CLS] [SEP])
    tokens = tokenizer.convert_ids_to_tokens(
        input_ids[1:seq_len - 1])

    # 获取标签
    labels = preds[1:seq_len - 1].tolist()

    # 组合文本和标点
    text = ''
    for token, label in zip(tokens, labels):
        text += token
        if label != 0:  # 非空格标点
            text += punc_list[label]

    return text
```

### 1.6 使用示例

#### CLI 使用

```bash
# 基本用法
paddlespeech text --input "你好世界" --task punc

# 指定模型
paddlespeech text \
    --input "今天天气真好我们要去公园玩" \
    --task punc \
    --model ernie_linear_p7_wudao \
    --lang zh

# 输出: "今天天气真好，我们要去公园玩。"
```

#### Python API 使用

```python
from paddlespeech.cli.text import TextExecutor

# 初始化执行器
text_executor = TextExecutor()

# 标点恢复
result = text_executor(
    text="今天的天气怎么样我们去公园玩吧",
    task='punc',
    model='ernie_linear_p7_wudao',
    lang='zh',
    device='cpu'
)

print(result)
# 输出: "今天的天气怎么样？我们去公园玩吧。"
```

#### 批量处理

```python
texts = [
    "今天天气很好",
    "我们去公园玩吧",
    "你吃了吗"
]

results = []
for text in texts:
    result = text_executor(text=text, task='punc')
    results.append(result)

# results:
# ["今天天气很好。",
#  "我们去公园玩吧。",
#  "你吃了吗？"]
```

### 1.7 支持的模型版本

| 模型名称 | ERNIE 版本 | 训练数据 | 特点 |
|---------|-----------|---------|------|
| ernie_linear_p3_wudao | ERNIE 3.0 | WuDaoCorpus | 3 层，速度快 |
| ernie_linear_p7_wudao | ERNIE 3.0 | WuDaoCorpus | 7 层，效果好 |
| ernie_linear_p7_wudao_fast | ERNIE 3.0-mini | WuDaoCorpus | 快速版本 |
| ernie_linear_p3_wudao_fast | ERNIE 3.0-mini | WuDaoCorpus | 最快版本 |

### 1.8 训练自定义模型

**配置文件** (conf/punc_model.yaml):

```yaml
model:
    model_type: ErnieLinear
    pretrained_token: ernie-1.0
    num_classes: 8  # 标点类别数

data_params:
    train_path: data/train.txt
    dev_path: data/dev.txt
    punc_path: data/vocab/punc_vocab.txt
    pretrained_token: ernie-1.0
    seq_len: 100
    batch_size: 32
    num_workers: 4

optimization:
    learning_rate: 5e-5
    weight_decay: 0.0
    warmup_steps: 500
    epochs: 10
```

**训练命令**:

```bash
python paddlespeech/text/exps/ernie_linear/train.py \
    --config conf/punc_model.yaml \
    --output exp/ernie_linear
```

---

## 2. 文本规范化 (Text Normalization)

### 2.1 概述

文本规范化将书写形式的文本转换为口语形式，主要用于 TTS 前端处理，也可用于 ASR 后处理的文本格式统一。

**核心文件**:
- 主模块: `paddlespeech/t2s/frontend/zh_normalization/text_normlization.py`
- 数字处理: `paddlespeech/t2s/frontend/zh_normalization/num.py`
- 日期时间: `paddlespeech/t2s/frontend/zh_normalization/chronology.py`
- 计量单位: `paddlespeech/t2s/frontend/zh_normalization/quantifier.py`
- 电话号码: `paddlespeech/t2s/frontend/zh_normalization/phonecode.py`
- 字符转换: `paddlespeech/t2s/frontend/zh_normalization/char_convert.py`

### 2.2 TextNormalizer 类

```python
# paddlespeech/t2s/frontend/zh_normalization/text_normlization.py
class TextNormalizer:
    """
    中文文本规范化器

    功能:
        - 繁简转换
        - 全角半角转换
        - 数字转中文
        - 日期时间转换
        - 单位符号转换
    """
    def __init__(self):
        # 句子分割正则
        self.SENTENCE_SPLITOR = re.compile(
            r'([：、，；。？！,;?!]["']?)'
        )

    def normalize(self, text: str) -> List[str]:
        """
        主入口: 规范化文本
        """
        # 1. 分句
        sentences = self._split(text)
        # 2. 逐句规范化
        sentences = [self.normalize_sentence(sent)
                     for sent in sentences]
        return sentences
```

### 2.3 规范化规则详解

#### 2.3.1 数字规范化

**支持的数字类型**:

```python
# 1. 基础数字
DIGITS = {str(i): tran for i, tran in
          enumerate('零一二三四五六七八九')}

# 2. 单位
UNITS = OrderedDict({
    1: '十',
    2: '百',
    3: '千',
    4: '万',
    8: '亿',
})

# 3. 量词
COM_QUANTIFIERS = '(封|艘|把|目|套|段|人|所|朵|匹|张|座|...)'
```

**规则表达式**:

| 规则 | 正则表达式 | 示例 → 结果 |
|-----|-----------|-----------|
| 整数 | `RE_INTEGER = r'(-)(\d+)'` | `-123` → `负一百二十三` |
| 小数 | `RE_DECIMAL_NUM = r'(-?)((\d+)(\.\d+))'` | `3.14` → `三点一四` |
| 分数 | `RE_FRAC = r'(-?)(\d+)/(\d+)'` | `1/2` → `二分之一` |
| 百分数 | `RE_PERCENTAGE = r'(-?)(\d+(\.\d+)?)%'` | `50%` → `百分之五十` |
| 范围 | `RE_RANGE = r'(数字)[-~](数字)'` | `10~20` → `十到二十` |
| 编号 | `RE_DEFAULT_NUM = r'\d{3}\d*'` | `001` → `零零一` |
| 带量词 | `RE_POSITIVE_QUANTIFIERS` | `3个人` → `三个人` |

**数字转中文函数**:

```python
def num2str(value_string: str) -> str:
    """
    将数字字符串转换为中文读法

    示例:
        "123" → "一百二十三"
        "1001" → "一千零一"
        "0" → "零"
    """
    value_string = value_string.lstrip('0')
    if len(value_string) == 0:
        return '零'
    if len(value_string) == 1:
        return DIGITS[value_string]

    # 递归处理
    largest_unit = next(
        power for power in reversed(UNITS.keys())
        if power < len(value_string)
    )
    first_part = value_string[:-largest_unit]
    second_part = value_string[-largest_unit:]

    return num2str(first_part) + UNITS[largest_unit] + \
           num2str(second_part)

def verbalize_digit(value_string: str, alt_one=True) -> str:
    """
    逐位读数字 (用于编号等)

    示例:
        "123" → "一二三"
        "001" → "零零一"
    """
    result = []
    for char in value_string:
        result.append(DIGITS.get(char, char))
    return ''.join(result)
```

#### 2.3.2 日期时间规范化

**日期表达式**:

```python
# 中文日期格式: YYYY年MM月DD日
RE_DATE = re.compile(
    r'(\d{4}|\d{2})年'
    r'((0?[1-9]|1[0-2])月)?'
    r'(((0?[1-9])|((1|2)[0-9])|30|31)([日号]))?'
)

# 分隔日期格式: YYYY/MM/DD 或 YYYY-MM-DD
RE_DATE2 = re.compile(
    r'(\d{4})([- /.])(0[1-9]|1[012])\2(0[1-9]|[12][0-9]|3[01])'
)

def replace_date(match) -> str:
    """
    示例:
        "2023年12月25日" → "二零二三年十二月二十五日"
        "99年5月1日" → "九九年五月一日"
    """
    year = match.group(1)
    month = match.group(3)
    day = match.group(5)

    result = f"{verbalize_digit(year)}年"
    if month:
        result += f"{verbalize_cardinal(month)}月"
    if day:
        result += f"{verbalize_cardinal(day)}{match.group(9)}"
    return result
```

**时间表达式**:

```python
# 时间格式: HH:MM:SS
RE_TIME = re.compile(
    r'([0-1]?[0-9]|2[0-3])'     # 小时
    r':([0-5][0-9])'            # 分钟
    r'(:([0-5][0-9]))?'         # 秒 (可选)
)

# 时间范围: HH:MM-HH:MM
RE_TIME_RANGE = re.compile(
    r'([0-1]?[0-9]|2[0-3]):([0-5][0-9])(:([0-5][0-9]))?'
    r'(~|-)'
    r'([0-1]?[0-9]|2[0-3]):([0-5][0-9])(:([0-5][0-9]))?'
)

def replace_time(match) -> str:
    """
    示例:
        "8:30" → "八点三十"
        "14:00" → "十四点"
        "8:30-12:30" → "八点三十至十二点三十"
    """
    is_range = len(match.groups()) > 5

    hour = match.group(1)
    minute = match.group(2)
    second = match.group(4)

    result = f"{num2str(hour)}点"

    if minute.lstrip('0'):
        if int(minute) == 30:
            result += "半"
        else:
            result += f"{_time_num2str(minute)}分"

    if second and second.lstrip('0'):
        result += f"{_time_num2str(second)}秒"

    # 处理时间范围
    if is_range:
        result += "至"
        # ... 处理第二个时间

    return result
```

#### 2.3.3 温度和计量单位

```python
# paddlespeech/t2s/frontend/zh_normalization/quantifier.py

# 温度表达式
RE_TEMPERATURE = re.compile(
    r'(-?)(\d+(\.\d+)?)(°C|℃|度|摄氏度)'
)

measure_dict = {
    "cm": "厘米", "km": "千米", "m": "米", "mm": "毫米",
    "kg": "千克", "g": "克", "mg": "毫克",
    "ml": "毫升", "L": "升",
    "cm²": "平方厘米", "m²": "平方米",
    "cm³": "立方厘米", "m³": "立方米",
    # ... 更多单位
}

def replace_temperature(match) -> str:
    """
    示例:
        "25℃" → "二十五度"
        "-3℃" → "零下三度"
        "37.5摄氏度" → "三十七点五摄氏度"
    """
    sign = match.group(1)
    temperature = match.group(2)
    unit = match.group(3)

    sign_str = "零下" if sign else ""
    temp_str = num2str(temperature)
    unit_str = "摄氏度" if unit == "摄氏度" else "度"

    return f"{sign_str}{temp_str}{unit_str}"

def replace_measure(sentence) -> str:
    """
    替换计量单位
    """
    for notation, chinese in measure_dict.items():
        sentence = sentence.replace(notation, chinese)
    return sentence
```

#### 2.3.4 电话号码

```python
# paddlespeech/t2s/frontend/zh_normalization/phonecode.py

# 手机号
RE_MOBILE_PHONE = re.compile(
    r'((?:(?:\+|00)86)?1[3-9]\d{9})'
)

# 固定电话
RE_TELEPHONE = re.compile(
    r'((?:(?:\+|00)86)?[0-9-]{10,15})'
)

# 国家统一号码
RE_NATIONAL_UNIFORM_NUMBER = re.compile(
    r'((?:\+|00)86-[0-9-]{10,15})'
)

def replace_mobile(match) -> str:
    """
    示例:
        "13800138000" → "一三八零零一三八零零零"
        "+8613800138000" → "八六一三八零零一三八零零零"
    """
    number = match.group(1)
    # 去除国家代码
    number = number.replace('+86', '').replace('0086', '')
    # 逐位读
    return verbalize_digit(number)
```

#### 2.3.5 特殊字符替换

```python
def _post_replace(self, sentence: str) -> str:
    """
    后处理特殊字符

    包括:
        - 符号转换: / → 每, ~ → 至
        - 罗马数字: ①②③ → 一二三
        - 希腊字母: αβγ → 阿尔法贝塔伽玛
    """
    # 符号转换
    sentence = sentence.replace('/', '每')
    sentence = sentence.replace('~', '至')
    sentence = sentence.replace('～', '至')

    # 罗马数字
    sentence = sentence.replace('①', '一')
    sentence = sentence.replace('②', '二')
    # ... ⑩

    # 希腊字母
    sentence = sentence.replace('α', '阿尔法')
    sentence = sentence.replace('β', '贝塔')
    sentence = sentence.replace('γ', '伽玛')
    # ... Ω → 欧米伽

    # 清理特殊字符
    sentence = re.sub(r'[-——《》【】<=>{}()（）#&@""^_|...\\]', '', sentence)

    return sentence
```

### 2.4 规范化顺序

**重要**: 规范化的顺序很关键，需要按优先级处理

```python
def normalize_sentence(self, sentence: str) -> str:
    """
    按照正确顺序进行规范化
    """
    # 1. 基础字符转换
    sentence = tranditional_to_simplified(sentence)
    sentence = sentence.translate(F2H_ASCII_LETTERS)    # 全角字母
    sentence = sentence.translate(F2H_DIGITS)            # 全角数字
    sentence = sentence.translate(F2H_SPACE)             # 全角空格

    # 2. 日期时间 (优先处理范围)
    sentence = RE_DATE.sub(replace_date, sentence)
    sentence = RE_DATE2.sub(replace_date2, sentence)
    sentence = RE_TIME_RANGE.sub(replace_time, sentence)  # 先处理范围
    sentence = RE_TIME.sub(replace_time, sentence)

    # 3. 温度和计量
    sentence = RE_TEMPERATURE.sub(replace_temperature, sentence)
    sentence = replace_measure(sentence)

    # 4. 特殊格式数字
    sentence = RE_FRAC.sub(replace_frac, sentence)           # 分数
    sentence = RE_PERCENTAGE.sub(replace_percentage, sentence)  # 百分数
    sentence = RE_MOBILE_PHONE.sub(replace_mobile, sentence)     # 手机号
    sentence = RE_TELEPHONE.sub(replace_phone, sentence)         # 电话
    sentence = RE_NATIONAL_UNIFORM_NUMBER.sub(replace_phone, sentence)

    # 5. 常规数字 (放在最后)
    sentence = RE_RANGE.sub(replace_range, sentence)
    sentence = RE_INTEGER.sub(replace_negative_num, sentence)
    sentence = RE_DECIMAL_NUM.sub(replace_number, sentence)
    sentence = RE_POSITIVE_QUANTIFIERS.sub(replace_positive_quantifier, sentence)
    sentence = RE_DEFAULT_NUM.sub(replace_default_num, sentence)
    sentence = RE_NUMBER.sub(replace_number, sentence)

    # 6. 后处理
    sentence = self._post_replace(sentence)

    return sentence
```

### 2.5 使用示例

```python
from paddlespeech.t2s.frontend.zh_normalization import TextNormalizer

# 初始化
normalizer = TextNormalizer()

# 简单文本
text = "今天气温25℃"
result = normalizer.normalize(text)
print(result)
# 输出: ["今天气温二十五度"]

# 复杂文本
text = "会议时间：2023年12月25日14:30-16:00，参会人员约50人，会议室在3楼301室"
result = normalizer.normalize(text)
print(result)
# 输出: ["会议时间：二零二三年十二月二十五日十四点三十至十六点，参会人员约五十人，会议室在三楼三零一室"]

# 数字和单位
text = "身高175cm，体重65kg，温度37.5℃"
result = normalizer.normalize(text)
print(result)
# 输出: ["身高一百七十五厘米，体重六十五千克，温度三十七点五度"]

# 电话号码
text = "联系电话：13800138000"
result = normalizer.normalize(text)
print(result)
# 输出: ["联系电话：一三八零零一三八零零零"]
```

---

## 3. 文本纠错 (Text Correction)

### 3.1 概述

文本纠错是指使用语言模型对 ASR 输出进行后处理，纠正识别错误。

**实现方式**:
1. **N-gram 语言模型**: 使用 KenLM 进行打分重排
2. **WFST 解码**: 将声学模型、发音词典、语言模型集成到解码器中
3. **集束搜索重打分**: 使用更强的语言模型对 N-best 候选重新打分

### 3.2 常见的 ASR 错误类型

**错误示例** (来自 `examples/other/ngram_lm/s0/data/text_correct.txt`):

| 原文 | 错误 | 错误类型 |
|-----|------|---------|
| 应该 | 因该 | 同音字 |
| 货运 | 送赁 | 近音字 |
| 早上 | 好上 | 同音字 |
| 非常 | 飞常 | 同音字 |
| 就是 | 就时 | 近音字 |
| 洗得 | 洗的 | 语法错误 |
| 什么 | 什麼 | 繁简混用 |

**错误分类**:

1. **同音字错误**: 发音相同但字不同的替换
   - 例: "因该" → "应该"

2. **近音字错误**: 发音相近的混淆
   - 例: "送赁" → "货运"

3. **语法错误**: 词性或语法使用错误
   - 例: "洗的干净" → "洗得干净"

4. **标点错误**: 标点符号缺失或错误使用

### 3.3 KenLM 语言模型纠错

**KenLM**: 高效的 N-gram 语言模型库

**安装**:

```bash
# 编译 KenLM
cd tools
make kenlm

# 或使用预编译版本
pip install https://github.com/kpu/kenlm/archive/master.zip
```

**训练语言模型**:

```bash
# examples/other/ngram_lm/s0/local/build_zh_lm.sh

# 1. 准备语料
cat text_corpus.txt | \
    # 清理文本
    # 分词 (中文需要分词)
    > corpus.txt

# 2. 构建词汇表
lmplz --optimize 1 -w 5 -S 80% -T /tmp \
      -o 5 < corpus.txt > arpa.txt

# 3. 转换为二进制格式
build_binary arpa.txt lm.bin
```

**参数说明**:
- `-o 5`: 5-gram 模型
- `-w 5`: 限制词汇表大小
- `-S 80%`: 限制内存使用

### 3.4 CTC 解码器集成

**文件**: `third_party/ctc_decoders/scorer.cpp`

**Scorer 类**:

```cpp
class Scorer {
public:
    Scorer(double alpha, double beta,
           const std::string& lm_path,
           const std::vector<std::string>& vocabulary);

    // 计算语言模型分数
    double get_log_cond_prob(const std::vector<std::string>& words);

    // 计算完整分数 (包括长度惩罚)
    double get_sent_log_prob(const std::vector<std::string>& words);

private:
    double alpha_;  // 语言模型权重
    double beta_;   // 长度惩罚系数
    KenLMState* language_model_;
    int max_order_;       // N-gram 阶数
    bool is_banned_character_;  // 是否有禁止字符
};
```

**Python 接口**:

```python
# paddlespeech/s2t/decoders/ctcdecoder/scorer_deprecated.py
class Scorer:
    """
    语言模型打分器

    用于对解码路径进行重打分
    """
    def __init__(self, alpha, beta, model_path, vocabulary):
        """
        Args:
            alpha: 语言模型权重
            beta: 长度惩罚系数
            model_path: KenLM 模型路径
            vocabulary: 词汇表
        """
        self._scorer = swig_paddle_scorer(
            alpha, beta, model_path, vocabulary
        )

    def get_log_cond_prob(self, words):
        """计算条件概率 P(w_t | w_1, ..., w_{t-1})"""
        return self._scorer.get_log_cond_prob(words)

    def get_sent_log_prob(self, words):
        """计算完整句子分数"""
        return self._scorer.get_sent_log_prob(words)
```

### 3.5 集束搜索重打分

**流程**:

```
ASR 声学模型输出
    ↓
[第一次解码] CTC Greedy/Beam Search
    ↓
N-best 候选列表
    ├─ 候选 1: "今天天气很好"
    ├─ 候选 2: "今天天气很好玩"
    └─ 候选 3: "今天天气很好完"
    ↓
[语言模型重打分] KenLM
    ├─ LM_score(候选1) = -2.5
    ├─ LM_score(候选2) = -8.2
    └─ LM_score(候选3) = -10.1
    ↓
[选择最佳] 候选 1: "今天天气很好"
```

**Python 实现**:

```python
def rescore_with_lm(nbest_candidates, scorer, alpha=0.5, beta=1.5):
    """
    使用语言模型对 N-best 候选重新打分

    Args:
        nbest_candidates: [(text, am_score), ...]
        scorer: KenLM 打分器
        alpha: 语言模型权重
        beta: 长度惩罚系数

    Returns:
        最佳文本
    """
    best_score = float('-inf')
    best_text = None

    for text, am_score in nbest_candidates:
        # 分词
        words = list(text)  # 中文按字分

        # 计算语言模型分数
        lm_score = scorer.get_sent_log_prob(words)

        # 计算长度惩罚
        length_penalty = len(words) ** beta

        # 综合分数
        total_score = am_score + alpha * lm_score - length_penalty

        if total_score > best_score:
            best_score = total_score
            best_text = text

    return best_text
```

### 3.6 WFST 集成解码

**TLG 构建**: Token FST + Lexicon FST + Grammar FST

```bash
# utils/fst/make_tlg.sh

# 1. Token FST (字符级别的 FST)
# 2. Lexicon FST (发音词典)
# 3. Grammar FST (语言模型)

# 编译 TLG
fstrmepsilon lang/L.fst | \
    fstdeterminizestar | \
    fstminimizeencoded | \
    fstrmsymbols lang/words.txt | \
    fstarcsort --sort_type=ilabel > lang/LG.fst

fsttablecompose lang/LG.fst lang/G.fst | \
    fstdeterminizestar | \
    fstminimizeencoded | \
    fstrmsymbols lang/words.txt | \
    fstarcsort --sort_type=ilabel > lang/TLG.fst
```

**使用 TLG 解码**:

```python
# 在线解码时使用 TLG
decoder = CTCDecoder(
    alphabet=labels,
    beam_width=10,
    num_processes=4,
    blank_index=0,
    lm_path='lang/TLG.fst',  # WFST 语言模型
    alpha=0.5,
    beta=1.5
)

result = decoder.decode(audio_features)
```

### 3.7 使用示例

#### Python API

```python
from paddlespeech.s2t.decoders.ctcdecoder import Scorer

# 初始化打分器
scorer = Scorer(
    alpha=0.5,         # 语言模型权重
    beta=1.5,          # 长度惩罚
    model_path='zh_gigaword_5g.bin',  # KenLM 模型
    vocabulary=['我', '你', '他', '好', ...]  # 词汇表
)

# 计算分数
text = "今天天气很好"
words = list(text)  # ['今', '天', '天', '气', '很', '好']
score = scorer.get_sent_log_prob(words)
print(f"LM score: {score}")

# 重打分
candidates = [
    ("今天天气很好", -10.5),
    ("今天天气很好玩", -12.3),
    ("今天天气很好完", -15.2)
]

best = rescore_with_lm(candidates, scorer, alpha=0.5, beta=1.5)
print(f"Best: {best}")  # "今天天气很好"
```

#### CLI 工具

```bash
# 使用 KenLM 打分文本
python examples/other/ngram_lm/s0/local/kenlm_score_test.py \
    --lm_path zh_gigaword_5g.bin \
    --vocab_path vocab.txt \
    --input_text test.txt
```

---

## 4. 完整后处理流水线

### 4.1 流程图

```
ASR 原始输出
    ↓
┌─────────────────────────────────────────┐
│  文本纠错 (可选)                         │
│  ├─ 语言模型重打分                       │
│  ├─ N-best 重排                         │
│  └─ WFST 解码                           │
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│  文本规范化 (可选，用于 TTS)             │
│  ├─ 繁简转换                            │
│  ├─ 全角半角转换                        │
│  ├─ 数字转中文                          │
│  ├─ 日期时间转换                        │
│  └─ 特殊符号转换                        │
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│  标点符号恢复                           │
│  ├─ ErnieLinear 模型                    │
│  └─ 标点预测与插入                      │
└─────────────────────────────────────────┘
    ↓
最终文本输出
```

### 4.2 完整代码示例

```python
from paddlespeech.cli.asr import ASRExecutor
from paddlespeech.cli.text import TextExecutor
from paddlespeech.s2t.decoders.ctcdecoder import Scorer
from paddlespeech.t2s.frontend.zh_normalization import TextNormalizer

class ASRPostProcessor:
    """
    ASR 后处理器
    """
    def __init__(self,
                 use_punc=True,
                 use_normalization=False,
                 use_lm_rescore=False):
        # 标点恢复
        if use_punc:
            self.punc_executor = TextExecutor()

        # 文本规范化
        if use_normalization:
            self.text_normalizer = TextNormalizer()

        # 语言模型重打分
        if use_lm_rescore:
            self.lm_scorer = Scorer(
                alpha=0.5,
                beta=1.5,
                model_path='zh_gigaword_5g.bin',
                vocabulary=self._load_vocab()
            )

    def process(self, asr_output, nbest_candidates=None):
        """
        完整后处理流程
        """
        text = asr_output

        # 1. 语言模型重打分 (如果提供 N-best)
        if nbest_candidates and hasattr(self, 'lm_scorer'):
            text = self._rescore(nbest_candidates)

        # 2. 文本规范化 (可选)
        if hasattr(self, 'text_normalizer'):
            # 如果需要 TTS 输出
            normalized = self.text_normalizer.normalize(text)
            normalized_text = ''.join(normalized)
        else:
            normalized_text = text

        # 3. 标点符号恢复
        if hasattr(self, 'punc_executor'):
            result = self.punc_executor(
                text=normalized_text,
                task='punc',
                model='ernie_linear_p7_wudao'
            )
        else:
            result = normalized_text

        return result

    def _rescore(self, nbest_candidates):
        """语言模型重打分"""
        best_score = float('-inf')
        best_text = nbest_candidates[0][0]

        for text, am_score in nbest_candidates:
            words = list(text)
            lm_score = self.lm_scorer.get_sent_log_prob(words)
            length_penalty = len(words) ** 1.5
            total_score = am_score + 0.5 * lm_score - length_penalty

            if total_score > best_score:
                best_score = total_score
                best_text = text

        return best_text

# 使用示例
processor = ASRPostProcessor(
    use_punc=True,
    use_normalization=False,
    use_lm_rescore=False
)

# ASR 原始输出
asr_output = "今天天气很好我们去公园玩吧"

# 后处理
result = processor.process(asr_output)
print(result)
# 输出: "今天天气很好，我们去公园玩吧。"
```

### 4.3 配置文件

**后处理配置** (postprocess_config.yaml):

```yaml
postprocess:
  # 标点符号恢复
  punctuation:
    enable: true
    model: ernie_linear_p7_wudao
    lang: zh
    device: cpu

  # 文本规范化
  normalization:
    enable: false  # 一般 ASR 不需要，TTS 需要
    split_sentences: true

  # 语言模型重打分
  lm_rescore:
    enable: false
    alpha: 0.5
    beta: 1.5
    lm_path: zh_gigaword_5g.bin
    vocab_path: vocab.txt

# 输出格式
output:
  format: text  # text, json
  with_timestamps: false
  with_confidence: false
```

---

## 5. 总结

### 三种后处理技术对比

| 技术 | 主要功能 | 实现方式 | 使用场景 |
|-----|---------|---------|---------|
| **标点符号恢复** | 添加标点符号 | ErnieLinear 分类模型 | 所有 ASR 输出 |
| **文本规范化** | 文本格式统一 | 规则引擎 + 正则表达式 | TTS 前端处理 |
| **文本纠错** | 纠正识别错误 | N-gram 语言模型 | 专业领域/高精度需求 |

### 选择建议

1. **日常使用**: 仅使用标点符号恢复
2. **TTS 场景**: 标点恢复 + 文本规范化
3. **高精度场景**: 标点恢复 + 语言模型重打分
4. **专业领域**: 领域语言模型 + 自定义纠错规则

### 性能考虑

| 技术 | CPU 延迟 | GPU 延迟 | 模型大小 |
|-----|---------|---------|---------|
| 标点恢复 (ernie-1.0) | ~100ms | ~30ms | ~400MB |
| 标点恢复 (ernie-3.0-mini) | ~50ms | ~15ms | ~100MB |
| 文本规范化 | <5ms | <5ms | 0 |
| LM 重打分 (5-gram) | ~10ms | - | ~500MB |

---

## 参考资料

### 相关论文

1. **Punctuation Restoration**:
   - "Punctuation Restoration with Self-Attention and BERT" (2020)

2. **Text Normalization**:
   - "Neural Text Normalization: A Simple Grammatical Error Correction Model" (2021)

3. **LM Rescoring**:
   - "Shallow Fusion with Neural Language Models" (2017)

### 代码位置索引

| 功能 | 文件路径 |
|-----|---------|
| ErnieLinear 模型 | `paddlespeech/text/models/ernie_linear/ernie_linear.py` |
| 标点恢复推理 | `paddlespeech/text/exps/ernie_linear/punc_restore.py` |
| 文本规范化 | `paddlespeech/t2s/frontend/zh_normalization/text_normlization.py` |
| 数字处理 | `paddlespeech/t2s/frontend/zh_normalization/num.py` |
| KenLM 打分器 | `paddlespeech/s2t/decoders/ctcdecoder/scorer_deprecated.py` |
| CTC 解码器 | `third_party/ctc_decoders/scorer.cpp` |
| 错误示例 | `examples/other/ngram_lm/s0/data/text_correct.txt` |
